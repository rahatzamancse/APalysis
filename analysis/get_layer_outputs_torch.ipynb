{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import transformers\n",
    "from transformers import modeling_outputs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(*objs, indent=0):\n",
    "    spacing = ' ' * indent\n",
    "    for obj in objs:\n",
    "        if isinstance(obj, dict):\n",
    "            print(f\"{spacing}{{\")\n",
    "            for key, value in obj.items():\n",
    "                print(f\"{spacing}  {key}:\")\n",
    "                pretty_print(value, indent + 4)\n",
    "            print(f\"{spacing}}}\")\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            print(f\"{spacing}[\")\n",
    "            for item in obj:\n",
    "                pretty_print(item, indent + 4)\n",
    "            print(f\"{spacing}]\")\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            print(f\"{spacing}np.array[{obj.dtype}] (shape: {obj.shape})\")\n",
    "        elif isinstance(obj, torch.Tensor):\n",
    "            print(f\"{spacing}torch.Tensor[{obj.dtype}] (shape: {obj.shape})\")\n",
    "        elif isinstance(obj, torch.nn.Module):\n",
    "            print(f\"{spacing}torch.nn.Module[{obj.dtype}] (shape: {obj.shape})\")\n",
    "        elif isinstance(obj, transformers.tokenization_utils_base.BatchEncoding):\n",
    "            print(f\"{spacing}BatchEncoding (keys: {list(obj.keys())})\")\n",
    "            for key, value in obj.items():\n",
    "                print(f\"{spacing}  {key}:\")\n",
    "                pretty_print(value, indent + 4)\n",
    "        else:\n",
    "            print(f\"{spacing}{obj}\")\n",
    "            \n",
    "def visualize_with_pyvis(G, width=\"1920px\", height=\"1080px\", hierarchical=False, file_name=\"pytorch_model_graph.html\"):\n",
    "    # Create a PyVis network object\n",
    "    net = Network(notebook=False, directed=True, width=width, height=height)\n",
    "\n",
    "    # Add nodes and edges from the NetworkX graph\n",
    "    for node, data in G.nodes(data=True):\n",
    "        net.add_node(node, title=str(data), label=node)\n",
    "\n",
    "    for source, target, data in G.edges(data=True):\n",
    "        net.add_edge(source, target, title=str(data))\n",
    "\n",
    "    # Apply hierarchical layout\n",
    "    net.repulsion(\n",
    "        node_distance=120,\n",
    "        central_gravity=0.33,\n",
    "        spring_length=100,\n",
    "        spring_strength=0.10,\n",
    "        damping=0.95,\n",
    "    )\n",
    "\n",
    "    options = {}\n",
    "    if hierarchical:\n",
    "        options[\"layout\"] = {\n",
    "            \"hierarchical\": {\n",
    "                \"enabled\": True,\n",
    "                \"levelSeparation\": 250,\n",
    "                \"nodeSpacing\": 400,\n",
    "                \"treeSpacing\": 200,\n",
    "                \"blockShifting\": False,\n",
    "                \"edgeMinimization\": True,\n",
    "                \"parentCentralization\": True,\n",
    "                \"direction\": \"UD\",\n",
    "                \"sortMethod\": \"directed\",\n",
    "            },\n",
    "            \"physics\": {\n",
    "                \"enabled\": False,\n",
    "            }\n",
    "        }\n",
    "        net.set_options(json.dumps(options))\n",
    "\n",
    "    # net.show_buttons()\n",
    "\n",
    "    # Save the network to an HTML file\n",
    "    net.save_graph(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetWithBranch(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNetWithBranch, self).__init__()\n",
    "        \n",
    "        # Common layers before branching\n",
    "        self.seq = torch.nn.Sequential(\n",
    "            torch.nn.Linear(10, 20),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.inner_seq_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(15, 15),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.inner_seq_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(15, 15),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Branch 1\n",
    "        self.branch1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 15),\n",
    "            self.inner_seq_1,\n",
    "            self.inner_seq_2\n",
    "        )\n",
    "        \n",
    "        # Branch 2\n",
    "        self.branch2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 15),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Common layers after recombining\n",
    "        self.fc_combine = torch.nn.Linear(30, 10)\n",
    "        self.fc_output = torch.nn.Linear(10, 1)\n",
    "        \n",
    "        # ModuleList after recombination\n",
    "        self.module_list = torch.nn.ModuleList([\n",
    "            torch.nn.Linear(1, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(5, 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Common forward pass before branching\n",
    "        x = self.seq(x)\n",
    "        \n",
    "        # Branching\n",
    "        branch1_out = self.branch1(x)\n",
    "        branch2_out = self.branch2(x)\n",
    "        \n",
    "        # Concatenate the outputs of both branches\n",
    "        x = torch.cat([branch1_out, branch2_out], dim=1)\n",
    "        \n",
    "        # Pass through the combined layer\n",
    "        x = self.fc_combine(x)\n",
    "        x = self.fc_output(x)\n",
    "        \n",
    "        # ModuleList forward pass\n",
    "        for layer in self.module_list:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n",
    "vgg16.eval()\n",
    "inceptionv3 = models.inception_v3(weights=models.Inception_V3_Weights.DEFAULT)\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "gpt2.config.pad_token_id = gpt2.config.eos_token_id\n",
    "gpt2.eval()\n",
    "vit = transformers.AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "vit.eval()\n",
    "complex_net = ComplexNetWithBranch()\n",
    "complex_net.eval()\n",
    "\n",
    "imagenette = torchvision.datasets.ImageFolder('/home/insane/U/apalysis-evaluation/dataset-5')\n",
    "gpt2_custom = [\n",
    "    'Hello world, sdjfklasdj ksdj fskdj lfskdj fklsadjf lskadjf laskdjf klasdjf klasd jfklasdj fklasd jfklasd jflaksjdf klasdjf aklsdjf klasjd fklasdj fklasfj',\n",
    "    'I love you',\n",
    "    'This is my world',\n",
    "    'Hello, how are you? I am fine, thank you!',\n",
    "    'Tell me a story on the moon',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image from the imagenette dataset\n",
    "# image, label = imagenette[0]\n",
    "# image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 3, 224, 224]))\n",
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 3, 299, 299]))\n",
      "BatchEncoding (keys: ['input_ids', 'attention_mask'])\n",
      "  input_ids:\n",
      "torch.Tensor[torch.int64] (shape: torch.Size([5, 79]))\n",
      "4\n",
      "  attention_mask:\n",
      "torch.Tensor[torch.int64] (shape: torch.Size([5, 79]))\n",
      "4\n",
      "np.array[uint8] (shape: (25, 299, 299, 3))\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "sampled_images = torch.utils.data.Subset(imagenette, np.random.choice(len(imagenette), 10, replace=False).tolist())\n",
    "transformed_images = [transform(image) for image, _ in sampled_images]\n",
    "input_vgg16 = torch.stack(transformed_images)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "sampled_images = torch.utils.data.Subset(imagenette, np.random.choice(len(imagenette), 10, replace=False).tolist())\n",
    "transformed_images = [transform(image) for image, _ in sampled_images]\n",
    "input_inceptionv3 = torch.stack(transformed_images)\n",
    "\n",
    "input_complex_net = torch.randn(10, 10)\n",
    "\n",
    "def gpt2_preprocess(x):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    x = tokenizer(x, return_tensors='pt', padding=True, truncation=True)\n",
    "    return x\n",
    "\n",
    "input_gpt2 = gpt2_preprocess(gpt2_custom)\n",
    "input_vit = np.array([x for x, _ in imagenette])\n",
    "\n",
    "pretty_print(input_vgg16)\n",
    "pretty_print(input_inceptionv3)\n",
    "pretty_print(input_gpt2)\n",
    "pretty_print(input_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16\n",
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 1000]))\n",
      "inceptionv3\n",
      "[\n",
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 1000]))\n",
      "4\n",
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 1000]))\n",
      "4\n",
      "]\n",
      "complex_net\n",
      "torch.Tensor[torch.float32] (shape: torch.Size([10, 1]))\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    pretty_print(\"vgg16\", vgg16(input_vgg16))\n",
    "    pretty_print(\"inceptionv3\", inceptionv3(input_inceptionv3))\n",
    "    # pretty_print(\"gpt2\", gpt2(input_gpt2))\n",
    "    # pretty_print(\"vit\", vit(input_vit))\n",
    "    pretty_print(\"complex_net\", complex_net(input_complex_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'features', 'features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8', 'features.9', 'features.10', 'features.11', 'features.12', 'features.13', 'features.14', 'features.15', 'features.16', 'features.17', 'features.18', 'features.19', 'features.20', 'features.21', 'features.22', 'features.23', 'features.24', 'features.25', 'features.26', 'features.27', 'features.28', 'features.29', 'features.30', 'avgpool', 'classifier', 'classifier.0', 'classifier.1', 'classifier.2', 'classifier.3', 'classifier.4', 'classifier.5', 'classifier.6']\n",
      "['', 'Conv2d_1a_3x3', 'Conv2d_1a_3x3.conv', 'Conv2d_1a_3x3.bn', 'Conv2d_2a_3x3', 'Conv2d_2a_3x3.conv', 'Conv2d_2a_3x3.bn', 'Conv2d_2b_3x3', 'Conv2d_2b_3x3.conv', 'Conv2d_2b_3x3.bn', 'maxpool1', 'Conv2d_3b_1x1', 'Conv2d_3b_1x1.conv', 'Conv2d_3b_1x1.bn', 'Conv2d_4a_3x3', 'Conv2d_4a_3x3.conv', 'Conv2d_4a_3x3.bn', 'maxpool2', 'Mixed_5b', 'Mixed_5b.branch1x1', 'Mixed_5b.branch1x1.conv', 'Mixed_5b.branch1x1.bn', 'Mixed_5b.branch5x5_1', 'Mixed_5b.branch5x5_1.conv', 'Mixed_5b.branch5x5_1.bn', 'Mixed_5b.branch5x5_2', 'Mixed_5b.branch5x5_2.conv', 'Mixed_5b.branch5x5_2.bn', 'Mixed_5b.branch3x3dbl_1', 'Mixed_5b.branch3x3dbl_1.conv', 'Mixed_5b.branch3x3dbl_1.bn', 'Mixed_5b.branch3x3dbl_2', 'Mixed_5b.branch3x3dbl_2.conv', 'Mixed_5b.branch3x3dbl_2.bn', 'Mixed_5b.branch3x3dbl_3', 'Mixed_5b.branch3x3dbl_3.conv', 'Mixed_5b.branch3x3dbl_3.bn', 'Mixed_5b.branch_pool', 'Mixed_5b.branch_pool.conv', 'Mixed_5b.branch_pool.bn', 'Mixed_5c', 'Mixed_5c.branch1x1', 'Mixed_5c.branch1x1.conv', 'Mixed_5c.branch1x1.bn', 'Mixed_5c.branch5x5_1', 'Mixed_5c.branch5x5_1.conv', 'Mixed_5c.branch5x5_1.bn', 'Mixed_5c.branch5x5_2', 'Mixed_5c.branch5x5_2.conv', 'Mixed_5c.branch5x5_2.bn', 'Mixed_5c.branch3x3dbl_1', 'Mixed_5c.branch3x3dbl_1.conv', 'Mixed_5c.branch3x3dbl_1.bn', 'Mixed_5c.branch3x3dbl_2', 'Mixed_5c.branch3x3dbl_2.conv', 'Mixed_5c.branch3x3dbl_2.bn', 'Mixed_5c.branch3x3dbl_3', 'Mixed_5c.branch3x3dbl_3.conv', 'Mixed_5c.branch3x3dbl_3.bn', 'Mixed_5c.branch_pool', 'Mixed_5c.branch_pool.conv', 'Mixed_5c.branch_pool.bn', 'Mixed_5d', 'Mixed_5d.branch1x1', 'Mixed_5d.branch1x1.conv', 'Mixed_5d.branch1x1.bn', 'Mixed_5d.branch5x5_1', 'Mixed_5d.branch5x5_1.conv', 'Mixed_5d.branch5x5_1.bn', 'Mixed_5d.branch5x5_2', 'Mixed_5d.branch5x5_2.conv', 'Mixed_5d.branch5x5_2.bn', 'Mixed_5d.branch3x3dbl_1', 'Mixed_5d.branch3x3dbl_1.conv', 'Mixed_5d.branch3x3dbl_1.bn', 'Mixed_5d.branch3x3dbl_2', 'Mixed_5d.branch3x3dbl_2.conv', 'Mixed_5d.branch3x3dbl_2.bn', 'Mixed_5d.branch3x3dbl_3', 'Mixed_5d.branch3x3dbl_3.conv', 'Mixed_5d.branch3x3dbl_3.bn', 'Mixed_5d.branch_pool', 'Mixed_5d.branch_pool.conv', 'Mixed_5d.branch_pool.bn', 'Mixed_6a', 'Mixed_6a.branch3x3', 'Mixed_6a.branch3x3.conv', 'Mixed_6a.branch3x3.bn', 'Mixed_6a.branch3x3dbl_1', 'Mixed_6a.branch3x3dbl_1.conv', 'Mixed_6a.branch3x3dbl_1.bn', 'Mixed_6a.branch3x3dbl_2', 'Mixed_6a.branch3x3dbl_2.conv', 'Mixed_6a.branch3x3dbl_2.bn', 'Mixed_6a.branch3x3dbl_3', 'Mixed_6a.branch3x3dbl_3.conv', 'Mixed_6a.branch3x3dbl_3.bn', 'Mixed_6b', 'Mixed_6b.branch1x1', 'Mixed_6b.branch1x1.conv', 'Mixed_6b.branch1x1.bn', 'Mixed_6b.branch7x7_1', 'Mixed_6b.branch7x7_1.conv', 'Mixed_6b.branch7x7_1.bn', 'Mixed_6b.branch7x7_2', 'Mixed_6b.branch7x7_2.conv', 'Mixed_6b.branch7x7_2.bn', 'Mixed_6b.branch7x7_3', 'Mixed_6b.branch7x7_3.conv', 'Mixed_6b.branch7x7_3.bn', 'Mixed_6b.branch7x7dbl_1', 'Mixed_6b.branch7x7dbl_1.conv', 'Mixed_6b.branch7x7dbl_1.bn', 'Mixed_6b.branch7x7dbl_2', 'Mixed_6b.branch7x7dbl_2.conv', 'Mixed_6b.branch7x7dbl_2.bn', 'Mixed_6b.branch7x7dbl_3', 'Mixed_6b.branch7x7dbl_3.conv', 'Mixed_6b.branch7x7dbl_3.bn', 'Mixed_6b.branch7x7dbl_4', 'Mixed_6b.branch7x7dbl_4.conv', 'Mixed_6b.branch7x7dbl_4.bn', 'Mixed_6b.branch7x7dbl_5', 'Mixed_6b.branch7x7dbl_5.conv', 'Mixed_6b.branch7x7dbl_5.bn', 'Mixed_6b.branch_pool', 'Mixed_6b.branch_pool.conv', 'Mixed_6b.branch_pool.bn', 'Mixed_6c', 'Mixed_6c.branch1x1', 'Mixed_6c.branch1x1.conv', 'Mixed_6c.branch1x1.bn', 'Mixed_6c.branch7x7_1', 'Mixed_6c.branch7x7_1.conv', 'Mixed_6c.branch7x7_1.bn', 'Mixed_6c.branch7x7_2', 'Mixed_6c.branch7x7_2.conv', 'Mixed_6c.branch7x7_2.bn', 'Mixed_6c.branch7x7_3', 'Mixed_6c.branch7x7_3.conv', 'Mixed_6c.branch7x7_3.bn', 'Mixed_6c.branch7x7dbl_1', 'Mixed_6c.branch7x7dbl_1.conv', 'Mixed_6c.branch7x7dbl_1.bn', 'Mixed_6c.branch7x7dbl_2', 'Mixed_6c.branch7x7dbl_2.conv', 'Mixed_6c.branch7x7dbl_2.bn', 'Mixed_6c.branch7x7dbl_3', 'Mixed_6c.branch7x7dbl_3.conv', 'Mixed_6c.branch7x7dbl_3.bn', 'Mixed_6c.branch7x7dbl_4', 'Mixed_6c.branch7x7dbl_4.conv', 'Mixed_6c.branch7x7dbl_4.bn', 'Mixed_6c.branch7x7dbl_5', 'Mixed_6c.branch7x7dbl_5.conv', 'Mixed_6c.branch7x7dbl_5.bn', 'Mixed_6c.branch_pool', 'Mixed_6c.branch_pool.conv', 'Mixed_6c.branch_pool.bn', 'Mixed_6d', 'Mixed_6d.branch1x1', 'Mixed_6d.branch1x1.conv', 'Mixed_6d.branch1x1.bn', 'Mixed_6d.branch7x7_1', 'Mixed_6d.branch7x7_1.conv', 'Mixed_6d.branch7x7_1.bn', 'Mixed_6d.branch7x7_2', 'Mixed_6d.branch7x7_2.conv', 'Mixed_6d.branch7x7_2.bn', 'Mixed_6d.branch7x7_3', 'Mixed_6d.branch7x7_3.conv', 'Mixed_6d.branch7x7_3.bn', 'Mixed_6d.branch7x7dbl_1', 'Mixed_6d.branch7x7dbl_1.conv', 'Mixed_6d.branch7x7dbl_1.bn', 'Mixed_6d.branch7x7dbl_2', 'Mixed_6d.branch7x7dbl_2.conv', 'Mixed_6d.branch7x7dbl_2.bn', 'Mixed_6d.branch7x7dbl_3', 'Mixed_6d.branch7x7dbl_3.conv', 'Mixed_6d.branch7x7dbl_3.bn', 'Mixed_6d.branch7x7dbl_4', 'Mixed_6d.branch7x7dbl_4.conv', 'Mixed_6d.branch7x7dbl_4.bn', 'Mixed_6d.branch7x7dbl_5', 'Mixed_6d.branch7x7dbl_5.conv', 'Mixed_6d.branch7x7dbl_5.bn', 'Mixed_6d.branch_pool', 'Mixed_6d.branch_pool.conv', 'Mixed_6d.branch_pool.bn', 'Mixed_6e', 'Mixed_6e.branch1x1', 'Mixed_6e.branch1x1.conv', 'Mixed_6e.branch1x1.bn', 'Mixed_6e.branch7x7_1', 'Mixed_6e.branch7x7_1.conv', 'Mixed_6e.branch7x7_1.bn', 'Mixed_6e.branch7x7_2', 'Mixed_6e.branch7x7_2.conv', 'Mixed_6e.branch7x7_2.bn', 'Mixed_6e.branch7x7_3', 'Mixed_6e.branch7x7_3.conv', 'Mixed_6e.branch7x7_3.bn', 'Mixed_6e.branch7x7dbl_1', 'Mixed_6e.branch7x7dbl_1.conv', 'Mixed_6e.branch7x7dbl_1.bn', 'Mixed_6e.branch7x7dbl_2', 'Mixed_6e.branch7x7dbl_2.conv', 'Mixed_6e.branch7x7dbl_2.bn', 'Mixed_6e.branch7x7dbl_3', 'Mixed_6e.branch7x7dbl_3.conv', 'Mixed_6e.branch7x7dbl_3.bn', 'Mixed_6e.branch7x7dbl_4', 'Mixed_6e.branch7x7dbl_4.conv', 'Mixed_6e.branch7x7dbl_4.bn', 'Mixed_6e.branch7x7dbl_5', 'Mixed_6e.branch7x7dbl_5.conv', 'Mixed_6e.branch7x7dbl_5.bn', 'Mixed_6e.branch_pool', 'Mixed_6e.branch_pool.conv', 'Mixed_6e.branch_pool.bn', 'AuxLogits', 'AuxLogits.conv0', 'AuxLogits.conv0.conv', 'AuxLogits.conv0.bn', 'AuxLogits.conv1', 'AuxLogits.conv1.conv', 'AuxLogits.conv1.bn', 'AuxLogits.fc', 'Mixed_7a', 'Mixed_7a.branch3x3_1', 'Mixed_7a.branch3x3_1.conv', 'Mixed_7a.branch3x3_1.bn', 'Mixed_7a.branch3x3_2', 'Mixed_7a.branch3x3_2.conv', 'Mixed_7a.branch3x3_2.bn', 'Mixed_7a.branch7x7x3_1', 'Mixed_7a.branch7x7x3_1.conv', 'Mixed_7a.branch7x7x3_1.bn', 'Mixed_7a.branch7x7x3_2', 'Mixed_7a.branch7x7x3_2.conv', 'Mixed_7a.branch7x7x3_2.bn', 'Mixed_7a.branch7x7x3_3', 'Mixed_7a.branch7x7x3_3.conv', 'Mixed_7a.branch7x7x3_3.bn', 'Mixed_7a.branch7x7x3_4', 'Mixed_7a.branch7x7x3_4.conv', 'Mixed_7a.branch7x7x3_4.bn', 'Mixed_7b', 'Mixed_7b.branch1x1', 'Mixed_7b.branch1x1.conv', 'Mixed_7b.branch1x1.bn', 'Mixed_7b.branch3x3_1', 'Mixed_7b.branch3x3_1.conv', 'Mixed_7b.branch3x3_1.bn', 'Mixed_7b.branch3x3_2a', 'Mixed_7b.branch3x3_2a.conv', 'Mixed_7b.branch3x3_2a.bn', 'Mixed_7b.branch3x3_2b', 'Mixed_7b.branch3x3_2b.conv', 'Mixed_7b.branch3x3_2b.bn', 'Mixed_7b.branch3x3dbl_1', 'Mixed_7b.branch3x3dbl_1.conv', 'Mixed_7b.branch3x3dbl_1.bn', 'Mixed_7b.branch3x3dbl_2', 'Mixed_7b.branch3x3dbl_2.conv', 'Mixed_7b.branch3x3dbl_2.bn', 'Mixed_7b.branch3x3dbl_3a', 'Mixed_7b.branch3x3dbl_3a.conv', 'Mixed_7b.branch3x3dbl_3a.bn', 'Mixed_7b.branch3x3dbl_3b', 'Mixed_7b.branch3x3dbl_3b.conv', 'Mixed_7b.branch3x3dbl_3b.bn', 'Mixed_7b.branch_pool', 'Mixed_7b.branch_pool.conv', 'Mixed_7b.branch_pool.bn', 'Mixed_7c', 'Mixed_7c.branch1x1', 'Mixed_7c.branch1x1.conv', 'Mixed_7c.branch1x1.bn', 'Mixed_7c.branch3x3_1', 'Mixed_7c.branch3x3_1.conv', 'Mixed_7c.branch3x3_1.bn', 'Mixed_7c.branch3x3_2a', 'Mixed_7c.branch3x3_2a.conv', 'Mixed_7c.branch3x3_2a.bn', 'Mixed_7c.branch3x3_2b', 'Mixed_7c.branch3x3_2b.conv', 'Mixed_7c.branch3x3_2b.bn', 'Mixed_7c.branch3x3dbl_1', 'Mixed_7c.branch3x3dbl_1.conv', 'Mixed_7c.branch3x3dbl_1.bn', 'Mixed_7c.branch3x3dbl_2', 'Mixed_7c.branch3x3dbl_2.conv', 'Mixed_7c.branch3x3dbl_2.bn', 'Mixed_7c.branch3x3dbl_3a', 'Mixed_7c.branch3x3dbl_3a.conv', 'Mixed_7c.branch3x3dbl_3a.bn', 'Mixed_7c.branch3x3dbl_3b', 'Mixed_7c.branch3x3dbl_3b.conv', 'Mixed_7c.branch3x3dbl_3b.bn', 'Mixed_7c.branch_pool', 'Mixed_7c.branch_pool.conv', 'Mixed_7c.branch_pool.bn', 'avgpool', 'dropout', 'fc']\n",
      "['', 'wte', 'wpe', 'drop', 'h', 'h.0', 'h.0.ln_1', 'h.0.attn', 'h.0.attn.c_attn', 'h.0.attn.c_proj', 'h.0.attn.attn_dropout', 'h.0.attn.resid_dropout', 'h.0.ln_2', 'h.0.mlp', 'h.0.mlp.c_fc', 'h.0.mlp.c_proj', 'h.0.mlp.act', 'h.0.mlp.dropout', 'h.1', 'h.1.ln_1', 'h.1.attn', 'h.1.attn.c_attn', 'h.1.attn.c_proj', 'h.1.attn.attn_dropout', 'h.1.attn.resid_dropout', 'h.1.ln_2', 'h.1.mlp', 'h.1.mlp.c_fc', 'h.1.mlp.c_proj', 'h.1.mlp.act', 'h.1.mlp.dropout', 'h.2', 'h.2.ln_1', 'h.2.attn', 'h.2.attn.c_attn', 'h.2.attn.c_proj', 'h.2.attn.attn_dropout', 'h.2.attn.resid_dropout', 'h.2.ln_2', 'h.2.mlp', 'h.2.mlp.c_fc', 'h.2.mlp.c_proj', 'h.2.mlp.act', 'h.2.mlp.dropout', 'h.3', 'h.3.ln_1', 'h.3.attn', 'h.3.attn.c_attn', 'h.3.attn.c_proj', 'h.3.attn.attn_dropout', 'h.3.attn.resid_dropout', 'h.3.ln_2', 'h.3.mlp', 'h.3.mlp.c_fc', 'h.3.mlp.c_proj', 'h.3.mlp.act', 'h.3.mlp.dropout', 'h.4', 'h.4.ln_1', 'h.4.attn', 'h.4.attn.c_attn', 'h.4.attn.c_proj', 'h.4.attn.attn_dropout', 'h.4.attn.resid_dropout', 'h.4.ln_2', 'h.4.mlp', 'h.4.mlp.c_fc', 'h.4.mlp.c_proj', 'h.4.mlp.act', 'h.4.mlp.dropout', 'h.5', 'h.5.ln_1', 'h.5.attn', 'h.5.attn.c_attn', 'h.5.attn.c_proj', 'h.5.attn.attn_dropout', 'h.5.attn.resid_dropout', 'h.5.ln_2', 'h.5.mlp', 'h.5.mlp.c_fc', 'h.5.mlp.c_proj', 'h.5.mlp.act', 'h.5.mlp.dropout', 'h.6', 'h.6.ln_1', 'h.6.attn', 'h.6.attn.c_attn', 'h.6.attn.c_proj', 'h.6.attn.attn_dropout', 'h.6.attn.resid_dropout', 'h.6.ln_2', 'h.6.mlp', 'h.6.mlp.c_fc', 'h.6.mlp.c_proj', 'h.6.mlp.act', 'h.6.mlp.dropout', 'h.7', 'h.7.ln_1', 'h.7.attn', 'h.7.attn.c_attn', 'h.7.attn.c_proj', 'h.7.attn.attn_dropout', 'h.7.attn.resid_dropout', 'h.7.ln_2', 'h.7.mlp', 'h.7.mlp.c_fc', 'h.7.mlp.c_proj', 'h.7.mlp.act', 'h.7.mlp.dropout', 'h.8', 'h.8.ln_1', 'h.8.attn', 'h.8.attn.c_attn', 'h.8.attn.c_proj', 'h.8.attn.attn_dropout', 'h.8.attn.resid_dropout', 'h.8.ln_2', 'h.8.mlp', 'h.8.mlp.c_fc', 'h.8.mlp.c_proj', 'h.8.mlp.act', 'h.8.mlp.dropout', 'h.9', 'h.9.ln_1', 'h.9.attn', 'h.9.attn.c_attn', 'h.9.attn.c_proj', 'h.9.attn.attn_dropout', 'h.9.attn.resid_dropout', 'h.9.ln_2', 'h.9.mlp', 'h.9.mlp.c_fc', 'h.9.mlp.c_proj', 'h.9.mlp.act', 'h.9.mlp.dropout', 'h.10', 'h.10.ln_1', 'h.10.attn', 'h.10.attn.c_attn', 'h.10.attn.c_proj', 'h.10.attn.attn_dropout', 'h.10.attn.resid_dropout', 'h.10.ln_2', 'h.10.mlp', 'h.10.mlp.c_fc', 'h.10.mlp.c_proj', 'h.10.mlp.act', 'h.10.mlp.dropout', 'h.11', 'h.11.ln_1', 'h.11.attn', 'h.11.attn.c_attn', 'h.11.attn.c_proj', 'h.11.attn.attn_dropout', 'h.11.attn.resid_dropout', 'h.11.ln_2', 'h.11.mlp', 'h.11.mlp.c_fc', 'h.11.mlp.c_proj', 'h.11.mlp.act', 'h.11.mlp.dropout', 'ln_f']\n",
      "['', 'vit', 'vit.embeddings', 'vit.embeddings.patch_embeddings', 'vit.embeddings.patch_embeddings.projection', 'vit.embeddings.dropout', 'vit.encoder', 'vit.encoder.layer', 'vit.encoder.layer.0', 'vit.encoder.layer.0.attention', 'vit.encoder.layer.0.attention.attention', 'vit.encoder.layer.0.attention.attention.query', 'vit.encoder.layer.0.attention.attention.key', 'vit.encoder.layer.0.attention.attention.value', 'vit.encoder.layer.0.attention.attention.dropout', 'vit.encoder.layer.0.attention.output', 'vit.encoder.layer.0.attention.output.dense', 'vit.encoder.layer.0.attention.output.dropout', 'vit.encoder.layer.0.intermediate', 'vit.encoder.layer.0.intermediate.dense', 'vit.encoder.layer.0.intermediate.intermediate_act_fn', 'vit.encoder.layer.0.output', 'vit.encoder.layer.0.output.dense', 'vit.encoder.layer.0.output.dropout', 'vit.encoder.layer.0.layernorm_before', 'vit.encoder.layer.0.layernorm_after', 'vit.encoder.layer.1', 'vit.encoder.layer.1.attention', 'vit.encoder.layer.1.attention.attention', 'vit.encoder.layer.1.attention.attention.query', 'vit.encoder.layer.1.attention.attention.key', 'vit.encoder.layer.1.attention.attention.value', 'vit.encoder.layer.1.attention.attention.dropout', 'vit.encoder.layer.1.attention.output', 'vit.encoder.layer.1.attention.output.dense', 'vit.encoder.layer.1.attention.output.dropout', 'vit.encoder.layer.1.intermediate', 'vit.encoder.layer.1.intermediate.dense', 'vit.encoder.layer.1.intermediate.intermediate_act_fn', 'vit.encoder.layer.1.output', 'vit.encoder.layer.1.output.dense', 'vit.encoder.layer.1.output.dropout', 'vit.encoder.layer.1.layernorm_before', 'vit.encoder.layer.1.layernorm_after', 'vit.encoder.layer.2', 'vit.encoder.layer.2.attention', 'vit.encoder.layer.2.attention.attention', 'vit.encoder.layer.2.attention.attention.query', 'vit.encoder.layer.2.attention.attention.key', 'vit.encoder.layer.2.attention.attention.value', 'vit.encoder.layer.2.attention.attention.dropout', 'vit.encoder.layer.2.attention.output', 'vit.encoder.layer.2.attention.output.dense', 'vit.encoder.layer.2.attention.output.dropout', 'vit.encoder.layer.2.intermediate', 'vit.encoder.layer.2.intermediate.dense', 'vit.encoder.layer.2.intermediate.intermediate_act_fn', 'vit.encoder.layer.2.output', 'vit.encoder.layer.2.output.dense', 'vit.encoder.layer.2.output.dropout', 'vit.encoder.layer.2.layernorm_before', 'vit.encoder.layer.2.layernorm_after', 'vit.encoder.layer.3', 'vit.encoder.layer.3.attention', 'vit.encoder.layer.3.attention.attention', 'vit.encoder.layer.3.attention.attention.query', 'vit.encoder.layer.3.attention.attention.key', 'vit.encoder.layer.3.attention.attention.value', 'vit.encoder.layer.3.attention.attention.dropout', 'vit.encoder.layer.3.attention.output', 'vit.encoder.layer.3.attention.output.dense', 'vit.encoder.layer.3.attention.output.dropout', 'vit.encoder.layer.3.intermediate', 'vit.encoder.layer.3.intermediate.dense', 'vit.encoder.layer.3.intermediate.intermediate_act_fn', 'vit.encoder.layer.3.output', 'vit.encoder.layer.3.output.dense', 'vit.encoder.layer.3.output.dropout', 'vit.encoder.layer.3.layernorm_before', 'vit.encoder.layer.3.layernorm_after', 'vit.encoder.layer.4', 'vit.encoder.layer.4.attention', 'vit.encoder.layer.4.attention.attention', 'vit.encoder.layer.4.attention.attention.query', 'vit.encoder.layer.4.attention.attention.key', 'vit.encoder.layer.4.attention.attention.value', 'vit.encoder.layer.4.attention.attention.dropout', 'vit.encoder.layer.4.attention.output', 'vit.encoder.layer.4.attention.output.dense', 'vit.encoder.layer.4.attention.output.dropout', 'vit.encoder.layer.4.intermediate', 'vit.encoder.layer.4.intermediate.dense', 'vit.encoder.layer.4.intermediate.intermediate_act_fn', 'vit.encoder.layer.4.output', 'vit.encoder.layer.4.output.dense', 'vit.encoder.layer.4.output.dropout', 'vit.encoder.layer.4.layernorm_before', 'vit.encoder.layer.4.layernorm_after', 'vit.encoder.layer.5', 'vit.encoder.layer.5.attention', 'vit.encoder.layer.5.attention.attention', 'vit.encoder.layer.5.attention.attention.query', 'vit.encoder.layer.5.attention.attention.key', 'vit.encoder.layer.5.attention.attention.value', 'vit.encoder.layer.5.attention.attention.dropout', 'vit.encoder.layer.5.attention.output', 'vit.encoder.layer.5.attention.output.dense', 'vit.encoder.layer.5.attention.output.dropout', 'vit.encoder.layer.5.intermediate', 'vit.encoder.layer.5.intermediate.dense', 'vit.encoder.layer.5.intermediate.intermediate_act_fn', 'vit.encoder.layer.5.output', 'vit.encoder.layer.5.output.dense', 'vit.encoder.layer.5.output.dropout', 'vit.encoder.layer.5.layernorm_before', 'vit.encoder.layer.5.layernorm_after', 'vit.encoder.layer.6', 'vit.encoder.layer.6.attention', 'vit.encoder.layer.6.attention.attention', 'vit.encoder.layer.6.attention.attention.query', 'vit.encoder.layer.6.attention.attention.key', 'vit.encoder.layer.6.attention.attention.value', 'vit.encoder.layer.6.attention.attention.dropout', 'vit.encoder.layer.6.attention.output', 'vit.encoder.layer.6.attention.output.dense', 'vit.encoder.layer.6.attention.output.dropout', 'vit.encoder.layer.6.intermediate', 'vit.encoder.layer.6.intermediate.dense', 'vit.encoder.layer.6.intermediate.intermediate_act_fn', 'vit.encoder.layer.6.output', 'vit.encoder.layer.6.output.dense', 'vit.encoder.layer.6.output.dropout', 'vit.encoder.layer.6.layernorm_before', 'vit.encoder.layer.6.layernorm_after', 'vit.encoder.layer.7', 'vit.encoder.layer.7.attention', 'vit.encoder.layer.7.attention.attention', 'vit.encoder.layer.7.attention.attention.query', 'vit.encoder.layer.7.attention.attention.key', 'vit.encoder.layer.7.attention.attention.value', 'vit.encoder.layer.7.attention.attention.dropout', 'vit.encoder.layer.7.attention.output', 'vit.encoder.layer.7.attention.output.dense', 'vit.encoder.layer.7.attention.output.dropout', 'vit.encoder.layer.7.intermediate', 'vit.encoder.layer.7.intermediate.dense', 'vit.encoder.layer.7.intermediate.intermediate_act_fn', 'vit.encoder.layer.7.output', 'vit.encoder.layer.7.output.dense', 'vit.encoder.layer.7.output.dropout', 'vit.encoder.layer.7.layernorm_before', 'vit.encoder.layer.7.layernorm_after', 'vit.encoder.layer.8', 'vit.encoder.layer.8.attention', 'vit.encoder.layer.8.attention.attention', 'vit.encoder.layer.8.attention.attention.query', 'vit.encoder.layer.8.attention.attention.key', 'vit.encoder.layer.8.attention.attention.value', 'vit.encoder.layer.8.attention.attention.dropout', 'vit.encoder.layer.8.attention.output', 'vit.encoder.layer.8.attention.output.dense', 'vit.encoder.layer.8.attention.output.dropout', 'vit.encoder.layer.8.intermediate', 'vit.encoder.layer.8.intermediate.dense', 'vit.encoder.layer.8.intermediate.intermediate_act_fn', 'vit.encoder.layer.8.output', 'vit.encoder.layer.8.output.dense', 'vit.encoder.layer.8.output.dropout', 'vit.encoder.layer.8.layernorm_before', 'vit.encoder.layer.8.layernorm_after', 'vit.encoder.layer.9', 'vit.encoder.layer.9.attention', 'vit.encoder.layer.9.attention.attention', 'vit.encoder.layer.9.attention.attention.query', 'vit.encoder.layer.9.attention.attention.key', 'vit.encoder.layer.9.attention.attention.value', 'vit.encoder.layer.9.attention.attention.dropout', 'vit.encoder.layer.9.attention.output', 'vit.encoder.layer.9.attention.output.dense', 'vit.encoder.layer.9.attention.output.dropout', 'vit.encoder.layer.9.intermediate', 'vit.encoder.layer.9.intermediate.dense', 'vit.encoder.layer.9.intermediate.intermediate_act_fn', 'vit.encoder.layer.9.output', 'vit.encoder.layer.9.output.dense', 'vit.encoder.layer.9.output.dropout', 'vit.encoder.layer.9.layernorm_before', 'vit.encoder.layer.9.layernorm_after', 'vit.encoder.layer.10', 'vit.encoder.layer.10.attention', 'vit.encoder.layer.10.attention.attention', 'vit.encoder.layer.10.attention.attention.query', 'vit.encoder.layer.10.attention.attention.key', 'vit.encoder.layer.10.attention.attention.value', 'vit.encoder.layer.10.attention.attention.dropout', 'vit.encoder.layer.10.attention.output', 'vit.encoder.layer.10.attention.output.dense', 'vit.encoder.layer.10.attention.output.dropout', 'vit.encoder.layer.10.intermediate', 'vit.encoder.layer.10.intermediate.dense', 'vit.encoder.layer.10.intermediate.intermediate_act_fn', 'vit.encoder.layer.10.output', 'vit.encoder.layer.10.output.dense', 'vit.encoder.layer.10.output.dropout', 'vit.encoder.layer.10.layernorm_before', 'vit.encoder.layer.10.layernorm_after', 'vit.encoder.layer.11', 'vit.encoder.layer.11.attention', 'vit.encoder.layer.11.attention.attention', 'vit.encoder.layer.11.attention.attention.query', 'vit.encoder.layer.11.attention.attention.key', 'vit.encoder.layer.11.attention.attention.value', 'vit.encoder.layer.11.attention.attention.dropout', 'vit.encoder.layer.11.attention.output', 'vit.encoder.layer.11.attention.output.dense', 'vit.encoder.layer.11.attention.output.dropout', 'vit.encoder.layer.11.intermediate', 'vit.encoder.layer.11.intermediate.dense', 'vit.encoder.layer.11.intermediate.intermediate_act_fn', 'vit.encoder.layer.11.output', 'vit.encoder.layer.11.output.dense', 'vit.encoder.layer.11.output.dropout', 'vit.encoder.layer.11.layernorm_before', 'vit.encoder.layer.11.layernorm_after', 'vit.layernorm', 'classifier']\n",
      "['', 'seq', 'seq.0', 'seq.1', 'inner_seq_1', 'inner_seq_1.0', 'inner_seq_1.1', 'inner_seq_2', 'inner_seq_2.0', 'inner_seq_2.1', 'branch1', 'branch1.0', 'branch2', 'branch2.0', 'branch2.1', 'fc_combine', 'fc_output', 'module_list', 'module_list.0', 'module_list.1', 'module_list.2']\n"
     ]
    }
   ],
   "source": [
    "vgg16_layers = list(\n",
    "    map(\n",
    "        lambda l: l[0],\n",
    "        vgg16.named_modules()\n",
    "    )\n",
    ")\n",
    "\n",
    "inceptionv3_layers = list(\n",
    "    map(\n",
    "        lambda l: l[0],\n",
    "        inceptionv3.named_modules()\n",
    "    )\n",
    ")\n",
    "\n",
    "gpt2_layers = list(\n",
    "    map(\n",
    "        lambda l: l[0],\n",
    "        gpt2.transformer.named_modules()\n",
    "    )\n",
    ")\n",
    "\n",
    "vit_layers = list(\n",
    "    map(\n",
    "        lambda l: l[0],\n",
    "        vit.named_modules()\n",
    "    )\n",
    ")\n",
    "\n",
    "complex_net_layers = list(\n",
    "    map(\n",
    "        lambda l: l[0],\n",
    "        complex_net.named_modules()\n",
    "    )\n",
    ")   \n",
    "\n",
    "print(vgg16_layers)\n",
    "print(inceptionv3_layers)\n",
    "print(gpt2_layers)\n",
    "print(vit_layers)\n",
    "print(complex_net_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding layer ComplexNetWithBranch_seq_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_module_list to ComplexNetWithBranch_seq_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_seq to ComplexNetWithBranch_seq_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_seq_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_seq_0 to ComplexNetWithBranch_seq_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_seq to ComplexNetWithBranch_seq_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_seq to the graph\n",
      "Adding edge from ComplexNetWithBranch_seq_1 to ComplexNetWithBranch_seq (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_seq (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_seq to ComplexNetWithBranch_branch1_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1 to ComplexNetWithBranch_branch1_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_1_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_0 to ComplexNetWithBranch_inner_seq_1_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_1 to ComplexNetWithBranch_inner_seq_1_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_1_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_1_0 to ComplexNetWithBranch_branch1_1_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1_1 to ComplexNetWithBranch_branch1_1_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_1_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_1_0 to ComplexNetWithBranch_inner_seq_1_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_1 to ComplexNetWithBranch_inner_seq_1_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_1_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_1_1 to ComplexNetWithBranch_branch1_1_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1_1 to ComplexNetWithBranch_branch1_1_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_1_1 to ComplexNetWithBranch_inner_seq_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_inner_seq_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_1 to ComplexNetWithBranch_branch1_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1 to ComplexNetWithBranch_branch1_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_2_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_1 to ComplexNetWithBranch_inner_seq_2_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_2 to ComplexNetWithBranch_inner_seq_2_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_2_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_2_0 to ComplexNetWithBranch_branch1_2_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1_2 to ComplexNetWithBranch_branch1_2_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_2_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_2_0 to ComplexNetWithBranch_inner_seq_2_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_2 to ComplexNetWithBranch_inner_seq_2_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_2_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_2_1 to ComplexNetWithBranch_branch1_2_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1_2 to ComplexNetWithBranch_branch1_2_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_inner_seq_2 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_2_1 to ComplexNetWithBranch_inner_seq_2 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_inner_seq_2 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1_2 to the graph\n",
      "Adding edge from ComplexNetWithBranch_inner_seq_2 to ComplexNetWithBranch_branch1_2 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch1 to ComplexNetWithBranch_branch1_2 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1_2 to ComplexNetWithBranch_branch1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_branch1 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch2_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch1 to ComplexNetWithBranch_branch2_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch2 to ComplexNetWithBranch_branch2_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch2_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch2_0 to ComplexNetWithBranch_branch2_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_branch2 to ComplexNetWithBranch_branch2_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_branch2 to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch2_1 to ComplexNetWithBranch_branch2 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_branch2 (parent)\n",
      "Adding layer ComplexNetWithBranch_fc_combine to the graph\n",
      "Adding edge from ComplexNetWithBranch_branch2 to ComplexNetWithBranch_fc_combine (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_fc_combine (parent)\n",
      "Adding layer ComplexNetWithBranch_fc_output to the graph\n",
      "Adding edge from ComplexNetWithBranch_fc_combine to ComplexNetWithBranch_fc_output (data_flow)\n",
      "Adding edge from ComplexNetWithBranch to ComplexNetWithBranch_fc_output (parent)\n",
      "Adding layer ComplexNetWithBranch_module_list_0 to the graph\n",
      "Adding edge from ComplexNetWithBranch_fc_output to ComplexNetWithBranch_module_list_0 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_module_list to ComplexNetWithBranch_module_list_0 (parent)\n",
      "Adding layer ComplexNetWithBranch_module_list_1 to the graph\n",
      "Adding edge from ComplexNetWithBranch_module_list_0 to ComplexNetWithBranch_module_list_1 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_module_list to ComplexNetWithBranch_module_list_1 (parent)\n",
      "Adding layer ComplexNetWithBranch_module_list_2 to the graph\n",
      "Adding edge from ComplexNetWithBranch_module_list_1 to ComplexNetWithBranch_module_list_2 (data_flow)\n",
      "Adding edge from ComplexNetWithBranch_module_list to ComplexNetWithBranch_module_list_2 (parent)\n",
      "Adding layer ComplexNetWithBranch to the graph\n",
      "Adding edge from ComplexNetWithBranch_module_list_2 to ComplexNetWithBranch (data_flow)\n",
      "Nodes not in model_graph_hierarchy: set()\n",
      "Nodes not in model_graph_dataflow: set()\n"
     ]
    }
   ],
   "source": [
    "# Function to create unique layer names\n",
    "def get_layer_name(module_name: str, parent_name: str | None = None) -> str:\n",
    "    if parent_name:\n",
    "        return f\"{parent_name}_{module_name}\"\n",
    "    return module_name\n",
    "\n",
    "# Hook function to add nodes and edges to the graph\n",
    "def add_to_graph(\n",
    "        G: nx.MultiDiGraph,\n",
    "        module: torch.nn.Module | torch.nn.ModuleList | torch.nn.Sequential | torch.nn.ModuleDict,\n",
    "        layer_name: str,\n",
    "        output: torch.Tensor | tuple[torch.Tensor, ...] | dict[str, torch.Tensor],\n",
    "        parent_name: str | None,\n",
    "        previous_layer_names: list[str]\n",
    "    ) -> None:\n",
    "    print(f\"Adding layer {layer_name} to the graph\")\n",
    "    # Get the layer type, dtype, and shape\n",
    "    layer_type = module.__class__.__name__\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    elif isinstance(output, dict):\n",
    "        if 'last_hidden_state' in output:\n",
    "            output = output['last_hidden_state']\n",
    "        elif 'logits' in output:\n",
    "            output = output['logits']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown output type: {output}\")\n",
    "    output_type = str(output.dtype)\n",
    "    output_shape = tuple(output.shape)\n",
    "\n",
    "    # Add node with attributes\n",
    "    G.add_node(\n",
    "        layer_name,\n",
    "        output=output,\n",
    "        output_type=output_type,\n",
    "        output_shape=output_shape,\n",
    "        layer_type=layer_type,\n",
    "    )\n",
    "\n",
    "    # Add edge from the previous layer to the current layer if it's not the input layer\n",
    "    if previous_layer_names:\n",
    "        for previous_layer_name in previous_layer_names:\n",
    "            print(f\"Adding edge from {previous_layer_name} to {layer_name} (data_flow)\")\n",
    "            G.add_edge(previous_layer_name, layer_name, edge_type=\"data_flow\")\n",
    "\n",
    "    if parent_name:\n",
    "        print(f\"Adding edge from {parent_name} to {layer_name} (parent)\")\n",
    "        G.add_edge(parent_name, layer_name, edge_type=\"parent\")\n",
    "\n",
    "def extract_activations_graph(model: torch.nn.Module | torch.nn.ModuleList | torch.nn.Sequential | torch.nn.ModuleDict, input_tensor: torch.Tensor | dict | transformers.tokenization_utils_base.BatchEncoding) -> nx.DiGraph:\n",
    "    G = nx.MultiDiGraph()\n",
    "    \n",
    "   # Stack to keep track of previous layers including branches\n",
    "    prev_layers_stack: list[list[str]] = []\n",
    "\n",
    "    # Function to recursively register hooks on all layers\n",
    "    def register_hooks(module: torch.nn.Module | torch.nn.ModuleList | torch.nn.Sequential | torch.nn.ModuleDict, module_name: str, parent_name: str | None) -> None:\n",
    "        def forward_hook(module, input, output):\n",
    "            previous_layer_names = prev_layers_stack.pop() if prev_layers_stack else []\n",
    "            add_to_graph(G, module, module_name, output, parent_name, previous_layer_names)\n",
    "            prev_layers_stack.append([module_name])\n",
    "\n",
    "        handle = module.register_forward_hook(forward_hook)\n",
    "\n",
    "        for name, child_module in module.named_children():\n",
    "            current_child_name = get_layer_name(name, module_name)\n",
    "            prev_layers_stack.append([module_name])\n",
    "            register_hooks(child_module, current_child_name, module_name)\n",
    "\n",
    "    # Register hooks starting from the root model\n",
    "    register_hooks(model, type(model).__name__, None)\n",
    "\n",
    "    try:\n",
    "        # Perform a forward pass with the input tensor to populate the graph\n",
    "        with torch.no_grad():\n",
    "            if isinstance(input_tensor, (dict, transformers.tokenization_utils_base.BatchEncoding)):\n",
    "                model(**input_tensor)\n",
    "            else:\n",
    "                model(input_tensor)\n",
    "    finally:\n",
    "        # remove all hooks\n",
    "        for module in model.modules():\n",
    "            module._forward_hooks.clear()\n",
    "            module._forward_pre_hooks.clear()\n",
    "    \n",
    "    G_hierarchy = G.copy()\n",
    "    edges_to_remove = [(u, v) for u, v, d in G_hierarchy.edges(data=True) if d.get(\"edge_type\") == \"data_flow\"]\n",
    "    G_hierarchy.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    G_dataflow = G.copy()\n",
    "    edges_to_remove = [(u, v) for u, v, d in G_dataflow.edges(data=True) if d.get(\"edge_type\") == \"parent\"]\n",
    "    G_dataflow.remove_edges_from(edges_to_remove)\n",
    "    \n",
    "    for node in G_hierarchy.nodes():\n",
    "        if G_hierarchy.out_degree(node) != 0:\n",
    "            child_nodes = [child for child in G_hierarchy.successors(node)]\n",
    "            for child in child_nodes:\n",
    "                if G_dataflow.out_degree(child) == 0:\n",
    "                    G.add_edge(child, node, edge_type=\"data_flow\")\n",
    "                else:\n",
    "                    next_child_nodes = [v for u, v, d in G_dataflow.edges(child, data=True)]\n",
    "                    if all(G_hierarchy.has_edge(child, next_child) for next_child in next_child_nodes):\n",
    "                        G.add_edge(child, node, edge_type=\"data_flow\")\n",
    "                        \n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_graph = extract_activations_graph(vgg16, input_vgg16)\n",
    "# model_graph = extract_activations_graph(inceptionv3, input_inceptionv3)\n",
    "model_graph = extract_activations_graph(complex_net, input_complex_net)\n",
    "visualize_with_pyvis(model_graph, file_name=\"whole_model.html\")\n",
    "\n",
    "model_graph_hierarchy = nx.DiGraph((u, v, d) for u, v, d in model_graph.edges(data=True) if d.get('edge_type') == 'parent')\n",
    "assert nx.is_directed_acyclic_graph(model_graph_hierarchy), \"The graph must be acyclic!\"\n",
    "nodes_not_in_hierarchy = set(model_graph.nodes()) - set(model_graph_hierarchy.nodes())\n",
    "print(\"Nodes not in model_graph_hierarchy:\", nodes_not_in_hierarchy)\n",
    "model_graph_hierarchy.add_nodes_from([(node, data) for node, data in model_graph.nodes(data=True) if node in nodes_not_in_hierarchy])\n",
    "visualize_with_pyvis(model_graph_hierarchy, file_name=\"whole_model_hierarchy.html\", hierarchical=True)\n",
    "\n",
    "model_graph_dataflow = nx.DiGraph((u, v, d) for u, v, d in model_graph.edges(data=True) if d['edge_type'] == 'data_flow')\n",
    "nodes_not_in_dataflow = set(model_graph.nodes()) - set(model_graph_dataflow.nodes())\n",
    "print(\"Nodes not in model_graph_dataflow:\", nodes_not_in_dataflow)\n",
    "model_graph_dataflow.add_nodes_from([(node, data) for node, data in model_graph.nodes(data=True) if node in nodes_not_in_dataflow])\n",
    "visualize_with_pyvis(model_graph_dataflow, file_name=\"whole_model_data_flow.html\", hierarchical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "\n",
    "def add_module_to_graph(module, parent_name, graph):\n",
    "    \"\"\"\n",
    "    Recursively add PyTorch module to a networkx graph, including hierarchical structure.\n",
    "    \n",
    "    Parameters:\n",
    "    - module: PyTorch module to process.\n",
    "    - parent_name: Name of the parent module (used to create hierarchical node names).\n",
    "    - graph: NetworkX graph to add nodes and edges to.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        child_name = f\"{parent_name}->{name}\"\n",
    "        graph.add_node(child_name, layer=type(child).__name__)  # Add child node\n",
    "\n",
    "        # Add a 'parent' edge for the hierarchy\n",
    "        graph.add_edge(parent_name, child_name, edge_type='parent')\n",
    "\n",
    "        # If the child is a Sequential or ModuleList, handle sublayers\n",
    "        if isinstance(child, (nn.Sequential, nn.ModuleList)):\n",
    "            for i, subchild in enumerate(child):\n",
    "                subchild_name = f\"{child_name}->{i}\"\n",
    "                graph.add_node(subchild_name, layer=type(subchild).__name__)  # Add sublayer node\n",
    "\n",
    "                # Add edges from the Sequential/ModuleList to sublayers\n",
    "                graph.add_edge(child_name, subchild_name, edge_type='parent')\n",
    "\n",
    "                # Recurse into sublayer if needed\n",
    "                add_module_to_graph(subchild, subchild_name, graph)\n",
    "        else:\n",
    "            # Recurse for standard layers\n",
    "            add_module_to_graph(child, child_name, graph)\n",
    "\n",
    "def hook_factory(name, graph, data_flow_edges, previous_layer_tracker):\n",
    "    \"\"\"\n",
    "    Create a hook function that logs the data flow from one layer to the next.\n",
    "    \n",
    "    Parameters:\n",
    "    - name: The name of the current layer.\n",
    "    - graph: The NetworkX graph being built.\n",
    "    - data_flow_edges: A list to log the data flow edges.\n",
    "    - previous_layer_tracker: A dictionary to globally track the previous layer across hooks.\n",
    "    \"\"\"\n",
    "    def hook(module, input, output):\n",
    "        previous_layer = previous_layer_tracker[\"prev\"]\n",
    "\n",
    "        # If there's a previous layer, add a data flow edge between previous and current layer\n",
    "        if previous_layer is not None:\n",
    "            data_flow_edges.append((previous_layer, name))\n",
    "        \n",
    "        # Update previous_layer to the current one\n",
    "        previous_layer_tracker[\"prev\"] = name\n",
    "        \n",
    "        return output\n",
    "    return hook\n",
    "\n",
    "def create_model_graph(model, input_tensor):\n",
    "    \"\"\"\n",
    "    Create a networkx graph from a PyTorch model, capturing both structure and data flow.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The PyTorch model instance to be processed.\n",
    "    - input_tensor: A tensor that can be passed through the model to trace the forward pass.\n",
    "    \n",
    "    Returns:\n",
    "    - graph: A NetworkX directed graph representing the model.\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()  # Use a directed graph for layer flow\n",
    "    data_flow_edges = []  # To track the flow of data (tensor passing)\n",
    "    previous_layer_tracker = {\"prev\": None}  # To track the previous layer globally\n",
    "\n",
    "    # Start from the root node\n",
    "    root_name = type(model).__name__\n",
    "    graph.add_node(root_name, layer=type(model).__name__)\n",
    "    \n",
    "    # Recursively add modules to the graph\n",
    "    add_module_to_graph(model, root_name, graph)\n",
    "    \n",
    "    # Register hooks on each layer to track the data flow\n",
    "    handles = []\n",
    "    for name, module in model.named_modules():\n",
    "        # Ignore the root module itself\n",
    "        if name != '':\n",
    "            # Use full name for the layer in the graph (matches hierarchy)\n",
    "            full_layer_name = f\"{root_name}->{name}\"\n",
    "            handle = module.register_forward_hook(hook_factory(full_layer_name, graph, data_flow_edges, previous_layer_tracker))\n",
    "            handles.append(handle)\n",
    "    \n",
    "    # Run a forward pass to log data flow\n",
    "    model(input_tensor)\n",
    "    \n",
    "    # Add the data flow edges to the graph\n",
    "    for (src, dst) in data_flow_edges:\n",
    "        if src in graph.nodes and dst in graph.nodes:\n",
    "            graph.add_edge(src, dst, edge_type='data_flow')\n",
    "    \n",
    "    # Remove hooks after tracing\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    return graph\n",
    "\n",
    "model = ComplexNetWithBranch()\n",
    "\n",
    "# Create a dummy input tensor matching the input size of the model (e.g., 10 features)\n",
    "input_tensor = torch.randn(1, 10)\n",
    "\n",
    "# Create the model graph\n",
    "model_graph = create_model_graph(model, input_tensor)\n",
    "\n",
    "visualize_with_pyvis(model_graph, file_name=\"whole_model_graph.html\", hierarchical=False)\n",
    "\n",
    "model_graph_dataflow = nx.DiGraph((u, v, d) for u, v, d in model_graph.edges(data=True) if d['edge_type'] == 'data_flow')\n",
    "nodes_to_remove = [node for node in model_graph_dataflow.nodes if node.isdigit() and len(node) > 5]\n",
    "\n",
    "for node in nodes_to_remove:\n",
    "    predecessors = list(model_graph_dataflow.predecessors(node))\n",
    "    successors = list(model_graph_dataflow.successors(node))\n",
    "    \n",
    "    for pred in predecessors:\n",
    "        for succ in successors:\n",
    "            model_graph_dataflow.add_edge(pred, succ, edge_type='data_flow')\n",
    "    \n",
    "    model_graph_dataflow.remove_node(node)\n",
    "\n",
    "visualize_with_pyvis(model_graph_dataflow, file_name=\"whole_model_data_flow.html\", hierarchical=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channelexplorer-ajWhp0a7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
