{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insane/.cache/pypoetry/virtualenvs/channelexplorer-ajWhp0a7-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_ids': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[15496,    11,   703,   389,   345,    30]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1]], dtype=int32)>},\n",
       " {'input_ids': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[  40,  716, 3734,   11, 5875,  345,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 7), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "def preprocess(texts: list[str]):\n",
    "    return [tokenizer(text, return_tensors='tf') for text in texts]\n",
    "\n",
    "inputs = preprocess([\"Hello, how are you?\", \"I am fine, thank you!\"])\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel at 0x722c3418fa30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFAutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insane/.cache/pypoetry/virtualenvs/channelexplorer-ajWhp0a7-py3.10/lib/python3.10/site-packages/transformers/generation/tf_utils.py:728: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/home/insane/.cache/pypoetry/virtualenvs/channelexplorer-ajWhp0a7-py3.10/lib/python3.10/site-packages/transformers/generation/tf_utils.py:837: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling output:  Hello, how are you?\n",
      "\n",
      "I am an English teacher. I am the first one to\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate(**inputs[0], do_sample=True, seed=(42, 0))\n",
    "print(\"Sampling output: \", tokenizer.decode(generated[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel at 0x722c3418fa30>,\n",
       " <transformers.models.gpt2.modeling_tf_gpt2.TFGPT2MainLayer at 0x722b73507fd0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_layers(model):\n",
    "    \"\"\"\n",
    "    Recursively retrieve all layers from a model.\n",
    "    \n",
    "    Args:\n",
    "    - model: TensorFlow or Keras model\n",
    "    \n",
    "    Returns:\n",
    "    - List of all layers in the model\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    \n",
    "    # Recursively add layers\n",
    "    def extract_layers(layer):\n",
    "        layers.append(layer)\n",
    "        # If the layer has sublayers, iterate over them\n",
    "        if hasattr(layer, 'layers'):\n",
    "            for sublayer in layer.layers:\n",
    "                extract_layers(sublayer)\n",
    "\n",
    "    # Start recursion with the input model\n",
    "    extract_layers(model)\n",
    "    return layers\n",
    "\n",
    "get_all_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[15496,    11,   703,   389,   345,    30]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 6), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1]], dtype=int32)>}\n",
      "embedding_layer: activation shape (1, 6, 768)\n",
      "block_0_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_0_feed_forward: activation shape (1, 6, 768)\n",
      "block_1_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_1_feed_forward: activation shape (1, 6, 768)\n",
      "block_2_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_2_feed_forward: activation shape (1, 6, 768)\n",
      "block_3_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_3_feed_forward: activation shape (1, 6, 768)\n",
      "block_4_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_4_feed_forward: activation shape (1, 6, 768)\n",
      "block_5_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_5_feed_forward: activation shape (1, 6, 768)\n",
      "block_6_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_6_feed_forward: activation shape (1, 6, 768)\n",
      "block_7_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_7_feed_forward: activation shape (1, 6, 768)\n",
      "block_8_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_8_feed_forward: activation shape (1, 6, 768)\n",
      "block_9_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_9_feed_forward: activation shape (1, 6, 768)\n",
      "block_10_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_10_feed_forward: activation shape (1, 6, 768)\n",
      "block_11_self_attention: activation shape (1, 12, 6, 6)\n",
      "block_11_feed_forward: activation shape (1, 6, 768)\n"
     ]
    }
   ],
   "source": [
    "# Function to extract activations from self-attention, feed-forward, and hidden states\n",
    "def get_transformer_block_activations(model, inputs):\n",
    "    \"\"\"\n",
    "    Function to get intermediate activations from each transformer block, including self-attention and feed-forward activations.\n",
    "    \n",
    "    Args:\n",
    "    - model: Transformer model from Hugging Face (e.g., GPT-2)\n",
    "    - inputs: Tokenized input (output of tokenizer)\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary containing activations for self-attention, feed-forward, and hidden states\n",
    "    \"\"\"\n",
    "    print(inputs)\n",
    "    # Run the model with output_hidden_states and output_attentions to get all intermediate data\n",
    "    outputs = model(input_ids=inputs['input_ids'], output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "    # The hidden_states contains the hidden layer outputs, and attentions contains attention scores\n",
    "    hidden_states = outputs.hidden_states\n",
    "    attentions = outputs.attentions\n",
    "\n",
    "    activations = {\n",
    "        \"embedding_layer\": hidden_states[0],  # First hidden state is the embedding layer\n",
    "    }\n",
    "\n",
    "    # Iterate over the transformer blocks\n",
    "    for i, (attention, hidden_state) in enumerate(zip(attentions, hidden_states[1:])):\n",
    "        # Self-attention output (stored in attentions)\n",
    "        activations[f\"block_{i}_self_attention\"] = attention\n",
    "        \n",
    "        # Feed-forward output (this is typically captured in the hidden states after attention and feed-forward pass)\n",
    "        activations[f\"block_{i}_feed_forward\"] = hidden_state\n",
    "\n",
    "    return activations\n",
    "\n",
    "# Tokenized inputs\n",
    "inputs = preprocess([\"Hello, how are you?\", \"I am fine, thank you!\"])\n",
    "\n",
    "# Process each input to get activations inside transformer blocks\n",
    "for input_data in inputs:\n",
    "    # Get the intermediate activations (self-attention, feed-forward layers) inside each block\n",
    "    block_activations = get_transformer_block_activations(model, input_data)\n",
    "    \n",
    "    # Display the activations for each component within each transformer block\n",
    "    for layer_name, activation in block_activations.items():\n",
    "        print(f\"{layer_name}: activation shape {activation.shape}\")\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "channelexplorer-ajWhp0a7-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
